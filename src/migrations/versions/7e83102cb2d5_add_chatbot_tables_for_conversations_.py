"""Add chatbot tables for conversations and messages

Revision ID: 7e83102cb2d5
Revises: 41f8fbb85bf9
Create Date: 2025-09-16 16:22:33.383568

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = '7e83102cb2d5'
down_revision: Union[str, None] = '41f8fbb85bf9'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('conversations',
    sa.Column('id', sa.UUID(), nullable=False),
    sa.Column('user_id', sa.Integer(), nullable=False),
    sa.Column('title', sa.String(length=255), nullable=True),
    sa.Column('created_at', sa.DateTime(timezone=True), nullable=False),
    sa.Column('updated_at', sa.DateTime(timezone=True), nullable=False),
    sa.ForeignKeyConstraint(['user_id'], ['user.id'], ),
    sa.PrimaryKeyConstraint('id'),
    sa.UniqueConstraint('id')
    )
    op.create_index(op.f('ix_conversations_user_id'), 'conversations', ['user_id'], unique=False)
    op.create_table('messages',
    sa.Column('id', sa.UUID(), nullable=False),
    sa.Column('conversation_id', sa.UUID(), nullable=False),
    sa.Column('role', sa.Enum('user', 'assistant', name='message_role'), nullable=False),
    sa.Column('content', sa.Text(), nullable=False),
    sa.Column('reasoning_steps', postgresql.JSONB(astext_type=sa.Text()), nullable=True),
    sa.Column('tool_calls', postgresql.JSONB(astext_type=sa.Text()), nullable=True),
    sa.Column('citations', postgresql.JSONB(astext_type=sa.Text()), nullable=True),
    sa.Column('processing_time', sa.Float(), nullable=True),
    sa.Column('confidence_score', sa.Float(), nullable=True),
    sa.Column('created_at', sa.DateTime(timezone=True), nullable=False),
    sa.ForeignKeyConstraint(['conversation_id'], ['conversations.id'], ),
    sa.PrimaryKeyConstraint('id'),
    sa.UniqueConstraint('id')
    )
    op.create_index(op.f('ix_messages_conversation_id'), 'messages', ['conversation_id'], unique=False)
    op.drop_index(op.f('idx_parent_collection'), table_name='parent_documents')
    op.drop_index(op.f('idx_parent_metadata'), table_name='parent_documents', postgresql_using='gin')
    op.drop_table('parent_documents')
    op.drop_table('langchain_pg_embedding')
    op.drop_index(op.f('ix_token_blacklist_token'), table_name='token_blacklist')
    op.drop_table('token_blacklist')
    op.drop_table('langchain_pg_collection')
    op.create_unique_constraint(None, 'rate_limit', ['id'])
    op.create_unique_constraint(None, 'stt_jobs', ['job_id'])
    op.create_unique_constraint(None, 'tier', ['id'])
    op.create_unique_constraint(None, 'transcript_segments', ['id'])
    op.create_unique_constraint(None, 'word_timestamps', ['id'])
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_constraint(None, 'word_timestamps', type_='unique')
    op.drop_constraint(None, 'transcript_segments', type_='unique')
    op.drop_constraint(None, 'tier', type_='unique')
    op.drop_constraint(None, 'stt_jobs', type_='unique')
    op.drop_constraint(None, 'rate_limit', type_='unique')
    op.create_table('langchain_pg_collection',
    sa.Column('name', sa.VARCHAR(), autoincrement=False, nullable=True),
    sa.Column('cmetadata', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True),
    sa.Column('uuid', sa.UUID(), autoincrement=False, nullable=False),
    sa.PrimaryKeyConstraint('uuid', name='langchain_pg_collection_pkey'),
    postgresql_ignore_search_path=False
    )
    op.create_table('token_blacklist',
    sa.Column('id', sa.INTEGER(), autoincrement=True, nullable=False),
    sa.Column('token', sa.VARCHAR(), autoincrement=False, nullable=False),
    sa.Column('expires_at', postgresql.TIMESTAMP(), autoincrement=False, nullable=False),
    sa.PrimaryKeyConstraint('id', name=op.f('token_blacklist_pkey'))
    )
    op.create_index(op.f('ix_token_blacklist_token'), 'token_blacklist', ['token'], unique=True)
    op.create_table('langchain_pg_embedding',
    sa.Column('collection_id', sa.UUID(), autoincrement=False, nullable=True),
    sa.Column('embedding', sa.NullType(), autoincrement=False, nullable=True),
    sa.Column('document', sa.VARCHAR(), autoincrement=False, nullable=True),
    sa.Column('cmetadata', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True),
    sa.Column('custom_id', sa.VARCHAR(), autoincrement=False, nullable=True),
    sa.Column('uuid', sa.UUID(), autoincrement=False, nullable=False),
    sa.ForeignKeyConstraint(['collection_id'], ['langchain_pg_collection.uuid'], name=op.f('langchain_pg_embedding_collection_id_fkey'), ondelete='CASCADE'),
    sa.PrimaryKeyConstraint('uuid', name=op.f('langchain_pg_embedding_pkey'))
    )
    op.create_table('parent_documents',
    sa.Column('id', sa.TEXT(), autoincrement=False, nullable=False),
    sa.Column('content', sa.TEXT(), autoincrement=False, nullable=False),
    sa.Column('metadata', postgresql.JSONB(astext_type=sa.Text()), autoincrement=False, nullable=True),
    sa.Column('collection_name', sa.VARCHAR(length=255), autoincrement=False, nullable=False),
    sa.Column('created_at', postgresql.TIMESTAMP(), server_default=sa.text('now()'), autoincrement=False, nullable=True),
    sa.Column('updated_at', postgresql.TIMESTAMP(), server_default=sa.text('now()'), autoincrement=False, nullable=True),
    sa.PrimaryKeyConstraint('id', name=op.f('parent_documents_pkey'))
    )
    op.create_index(op.f('idx_parent_metadata'), 'parent_documents', ['metadata'], unique=False, postgresql_using='gin')
    op.create_index(op.f('idx_parent_collection'), 'parent_documents', ['collection_name'], unique=False)
    op.drop_index(op.f('ix_messages_conversation_id'), table_name='messages')
    op.drop_table('messages')
    op.drop_index(op.f('ix_conversations_user_id'), table_name='conversations')
    op.drop_table('conversations')
    # ### end Alembic commands ###
